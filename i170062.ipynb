{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import textblob\n",
    "# pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = \"E:/UNI/DM/Labs/Lab09/large_movie_review_dataset/train/\"\n",
    "test_directory = \"E:/UNI/DM/Labs/Lab09/large_movie_review_dataset/test/\"\n",
    "\n",
    "def read_class(directory, klass):\n",
    "    '''\n",
    "    directory : directory path \"~train or ~test\"\n",
    "    klass : \"neg\" or \"pos\"\n",
    "\n",
    "    returns : list of stored reviews and their class\n",
    "    '''\n",
    "    reviews = []\n",
    "    os.chdir(directory+klass)\n",
    "    for file in glob.glob(\"*.txt\"):\n",
    "        with open(file, encoding=\"utf8\") as f:\n",
    "            data = f.read()\n",
    "            reviews.append([data.lower(), klass])\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training reviews from disk\n",
    "neg_reviews_train = read_class(train_directory, \"neg\")\n",
    "pos_reviews_train = read_class(train_directory, \"pos\")\n",
    "neg_df_tr = pd.DataFrame(neg_reviews_train)\n",
    "pos_df_tr = pd.DataFrame(pos_reviews_train)\n",
    "train_data = pos_df_tr.append(neg_df_tr, ignore_index=True)\n",
    "\n",
    "# Read Test reviews from disk\n",
    "neg_reviews_test = read_class(test_directory, \"neg\")\n",
    "pos_reviews_test = read_class(test_directory, \"pos\")\n",
    "neg_df_te = pd.DataFrame(neg_reviews_test)\n",
    "pos_df_te = pd.DataFrame(pos_reviews_test)\n",
    "test_data = pos_df_te.append(neg_df_te, ignore_index=True)\n",
    "\n",
    "# save a pickle of dataframes for easy loading\n",
    "\n",
    "directory = 'E:/UNI/DM/Labs/Lab09/'\n",
    "save_to_disk = (train_data, test_data)\n",
    "with open(directory+'test&traindata.pickle', 'wb') as f:\n",
    "    pickle.dump((save_to_disk), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Dataframe Pickles\n",
    "\n",
    "directory = 'E:/UNI/DM/Labs/Lab09/'\n",
    "with open(directory+'test&traindata.pickle', 'rb') as f:\n",
    "    train_data, test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickleObject():\n",
    "    '''\n",
    "    This Object will be saved to disk as a pickle after training\n",
    "    '''\n",
    "    def __init__(self, klasses, dicts, V, word_count, prob_klasses, rmv_stpwrd, alpha):\n",
    "        # class labels\n",
    "        self.klasses = klasses\n",
    "        # dictionaries\n",
    "        self.dicts = dicts\n",
    "        # Vocabulary Count\n",
    "        self.V = V\n",
    "        # Wordcount of each class\n",
    "        self.word_count = word_count\n",
    "        # prob of each class\n",
    "        self.prob_klasses = prob_klasses\n",
    "        # whether trained with or without stopwords\n",
    "        self.rmv_stpwrd = rmv_stpwrd\n",
    "        # alpha smoothing value\n",
    "        self.alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, data=None, load_from_disk=False, path=None):\n",
    "        '''\n",
    "        data = None or pandas DataFrame\n",
    "        load_from_disk = False(if data is given) or True\n",
    "        path = pickle name(will write this object with loaded pickle)\n",
    "        '''\n",
    "\n",
    "        if data is None and load_from_disk == False:\n",
    "            raise Exception(\n",
    "                'Either pass data or give a path to load dictionaries from disk')\n",
    "\n",
    "        if load_from_disk:\n",
    "            # Load Saved Dictionaries\n",
    "            if path == None:\n",
    "                raise Exception('Please enter a valid Path')\n",
    "            else:\n",
    "                self.load_dictionaries(path)\n",
    "\n",
    "        else:\n",
    "            self.trained = False\n",
    "            self.docs = data\n",
    "\n",
    "    # Private Method\n",
    "    def __tokenize(self, doc: str):\n",
    "        from collections import Counter\n",
    "        counts = Counter(doc.split())\n",
    "        return dict(counts)\n",
    "\n",
    "    # Private Method\n",
    "    def __remove_punctuation(self, doc: str):\n",
    "        \n",
    "        '''\n",
    "        remove punctuation marks in the given string\n",
    "        '''\n",
    "\n",
    "        punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')',\n",
    "                       '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?',\n",
    "                       '@', '[', '\\\\', 'br', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "        for pm in punctuation:\n",
    "            doc = doc.replace(pm, ' ')\n",
    "\n",
    "        return doc\n",
    "\n",
    "    # Private Method\n",
    "    def __remove_stopwords(self, doc: str):\n",
    "        \n",
    "        '''\n",
    "        remove Stop words in the given string\n",
    "        '''\n",
    "\n",
    "        from nltk.corpus import stopwords\n",
    "        words = stopwords.words('english')\n",
    "\n",
    "        for wrd in words:\n",
    "            doc = doc.replace(' '+wrd+' ', ' ')\n",
    "\n",
    "        return doc\n",
    "\n",
    "    def train_NB(self,alpha = 1, remove_stop_words=False):\n",
    "        '''\n",
    "        Trains Classifier\n",
    "\n",
    "        alpha : alpha smooting value, default is 1\n",
    "\n",
    "        remove_stop_words : whether to remove the stop words or not\n",
    "\n",
    "        returns : prior, |V|, list of dictionaries (words and their frequencies in each class)\n",
    "\n",
    "        '''\n",
    "        if self.docs is None:\n",
    "            raise Exception('Trained NB is loaded from disk.\\\n",
    "                                No documnets available for Training')\n",
    "        if alpha < 1:\n",
    "            raise Exception('Alpha should be >= 1')\n",
    "\n",
    "        # class Labels\n",
    "        self.klasses = list(self.docs[1].unique())\n",
    "        self.dicts = []\n",
    "        # Vocabulary count\n",
    "        self.V = 0\n",
    "        self.alpha = alpha\n",
    "        self.word_count = []\n",
    "        self.prob_klasses = []\n",
    "        self.rmv_stpwrd = remove_stop_words\n",
    "        strs = []\n",
    "\n",
    "        for klass in self.klasses:\n",
    "            docs = self.docs[self.docs[1] == klass][0].as_matrix()\n",
    "            self.prob_klasses.append(len(docs)/len(self.docs))\n",
    "\n",
    "            # Pre Processing text\n",
    "            doc_string = \" \".join(docs)\n",
    "            doc_string = self.__remove_punctuation(doc_string)\n",
    "            if remove_stop_words:\n",
    "                doc_string = self.__remove_stopwords(doc_string)\n",
    "\n",
    "            # make dictionary\n",
    "            dic = self.__tokenize(doc_string)\n",
    "            strs.append(doc_string)\n",
    "            self.word_count.append(len(doc_string.split()))\n",
    "            self.dicts.append(dic)\n",
    "\n",
    "        # Num of Unique words in all documents\n",
    "        self.V = len(set(\" \".join(strs).split()))\n",
    "        # Only save to disk if NB is trained\n",
    "        self.trained = True\n",
    "\n",
    "        return self.prob_klasses, self.V, self.dicts\n",
    "\n",
    "    def save_dictionaries(self, path):\n",
    "        '''\n",
    "        Saves dictionaries and other values required for prediction to the disk.\n",
    "\n",
    "        path : path of pickle file to be stored\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not self.trained:\n",
    "            raise Exception('!! Nothing to save !! Train First')\n",
    "\n",
    "        save_to_disk = PickleObject(self.klasses, self.dicts, self.V,\n",
    "        self.word_count, self.prob_klasses, self.rmv_stpwrd, self.alpha)\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(save_to_disk, f)\n",
    "\n",
    "    def load_dictionaries(self, path):\n",
    "        \n",
    "        '''\n",
    "        Loads dictionaries and other values required for prediction from the disk.\n",
    "\n",
    "        path : path of pickle file to be stored\n",
    "\n",
    "        '''\n",
    "\n",
    "        with open(path, 'rb') as f:\n",
    "            pickle_obj = pickle.load(f)\n",
    "\n",
    "        self.klasses = pickle_obj.klasses\n",
    "        self.dicts = pickle_obj.dicts\n",
    "        self.V = pickle_obj.V\n",
    "        self.word_count = pickle_obj.word_count\n",
    "        self.prob_klasses = pickle_obj.prob_klasses\n",
    "        self.rmv_stpwrd = pickle_obj.rmv_stpwrd\n",
    "        self.alpha = pickle_obj.alpha\n",
    "        self.trained = True\n",
    "\n",
    "    def predict(self, rev: str):\n",
    "\n",
    "        '''\n",
    "        Given a new review classify it into a class\n",
    "\n",
    "        rev : string to be classified\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Pre process the test review\n",
    "        rev = rev.lower()\n",
    "        rev = self.__remove_punctuation(rev)\n",
    "        if self.rmv_stpwrd:\n",
    "            rev = self.__remove_stopwords(rev)\n",
    "        uniq_words = set(rev.split())\n",
    "        dic = self.__tokenize(rev)\n",
    "\n",
    "        # list of probabilities that rev belongs to a klass\n",
    "        klass_scores = []\n",
    "        for idx, _ in enumerate(self.klasses):\n",
    "            # log of probability of a class\n",
    "            score = np.log(self.prob_klasses[idx])\n",
    "            for uw in uniq_words:\n",
    "                # if word is not in dictionary\n",
    "                if self.dicts[idx].get(uw) is None:\n",
    "                    # numerator\n",
    "                    num = self.alpha\n",
    "                else:\n",
    "                    num = self.dicts[idx].get(uw) + self.alpha\n",
    "                # denominator\n",
    "                den = self.word_count[idx] + (self.alpha * self.V)\n",
    "                power = dic.get(uw)\n",
    "                score += np.log((num/den) ** power)\n",
    "            klass_scores.append(score)\n",
    "        klass_index = np.argmax(klass_scores)\n",
    "        # return predicted class label\n",
    "        return self.klasses[klass_index]\n",
    "\n",
    "    def evaluate(self, df):\n",
    "\n",
    "        '''\n",
    "        Evaluate a given pandas dataframe\n",
    "\n",
    "        df : pandas dataframe\n",
    "\n",
    "        returns : accuracy of the classifier \n",
    "        '''\n",
    "\n",
    "        correct_preds = 0\n",
    "        klasses = list(df[1].unique())\n",
    "        for klass in klasses:\n",
    "\n",
    "            revs = list(df[df[1] == klass][0])\n",
    "            for rev in revs:\n",
    "                prediction = xb.predict(rev)\n",
    "\n",
    "                if prediction == klass:\n",
    "                    correct_preds += 1\n",
    "\n",
    "        return correct_preds / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# directory to store pickle\n",
    "direc = 'E:/UNI/DM/Labs/Lab09/'\n",
    "\n",
    "nb = NaiveBayes(data=train_data)\n",
    "nb.train_NB(alpha = 3)\n",
    "nb.save_dictionaries(direc + 'withsw.pickle')\n",
    "\n",
    "# Trained NB after removing Stop Words\n",
    "nbsw = NaiveBayes(data=train_data)\n",
    "nbsw.train_NB(alpha = 3, remove_stop_words=True)\n",
    "nbsw.save_dictionaries(direc + 'withoutsw.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with stop words :  0.81296\n",
      "Accuracy without stop words :  0.83296\n"
     ]
    }
   ],
   "source": [
    "direc = 'E:/UNI/DM/Labs/Lab09/'\n",
    "\n",
    "xb = NaiveBayes(load_from_disk=True, path=direc + 'withsw.pickle')\n",
    "\n",
    "print(\"Accuracy with stop words : \",xb.evaluate(test_data))\n",
    "\n",
    "\n",
    "xb = NaiveBayes(load_from_disk=True, path=direc + 'withoutsw.pickle')\n",
    "\n",
    "print(\"Accuracy without stop words : \",xb.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words\n",
      ".:    pos   neg\n",
      "\n",
      "1 :  film   movie\n",
      "2 :  movie   film\n",
      "3 :  one   one\n",
      "4 :  like   like\n",
      "5 :  good   even\n",
      "6 :  story   good\n",
      "7 :  time   bad\n",
      "8 :  great   would\n",
      "9 :  well   really\n",
      "10 :  see   time\n"
     ]
    }
   ],
   "source": [
    "pos_dict = xb.dicts[0]\n",
    "neg_dict = xb.dicts[1]\n",
    "sorted_pos_dict = {k: v for k, v in \n",
    "                        sorted(pos_dict.items(), key=lambda item: item[1], reverse= True)}\n",
    "\n",
    "sorted_neg_dict = {k: v for k, v in \n",
    "                        sorted(neg_dict.items(), key=lambda item: item[1], reverse= True)}\n",
    "\n",
    "top = 10\n",
    "neg_top = list(sorted_neg_dict.keys())[:top]\n",
    "pos_top = list(sorted_pos_dict.keys())[:top]\n",
    "print(\"Top {} words\".format(top))\n",
    "print(\".:    pos   neg\",end='\\n\\n')\n",
    "\n",
    "for i in range(top):\n",
    "    print(i+1,': ',pos_top[i],' ',neg_top[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationRetrieval:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __remove_stopwords(self, doc: str):\n",
    "        from nltk.corpus import stopwords\n",
    "        words = stopwords.words('english')\n",
    "        for wrd in words:\n",
    "            doc = doc.replace(' '+wrd+' ', ' ')\n",
    "        return doc\n",
    "\n",
    "    def __remove_punctuation(self, doc: str):\n",
    "        punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')',\n",
    "                       '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?',\n",
    "                       '@', '[', '\\\\', 'br', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "        for pm in punctuation:\n",
    "            doc = doc.replace(pm, ' ')\n",
    "        return doc\n",
    "\n",
    "    def vectorize_data(self):\n",
    "        '''\n",
    "        make W list containing all unique words\n",
    "        '''\n",
    "        doc_string = \" \".join(self.data)\n",
    "        doc_string = self.__remove_punctuation(doc_string)\n",
    "        doc_string = self.__remove_stopwords(doc_string)\n",
    "        self.W = list(set(doc_string.split()))\n",
    "\n",
    "    def __cosine_similarity(self, a, b):\n",
    "        '''\n",
    "        return cosine similarity b/w a and b\n",
    "        '''\n",
    "        return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "    def __bag_of_words_vector(self, doc: str):\n",
    "        '''\n",
    "        return bag_of_words_vector of doc based on W list\n",
    "        '''\n",
    "        \n",
    "        from collections import Counter\n",
    "\n",
    "        vec = np.zeros(len(self.W))\n",
    "        dic = dict(Counter(doc.split()))\n",
    "        for j in range (len(self.W)):\n",
    "            count = dic.get(self.W[j])\n",
    "            if count is None:\n",
    "                count = 0\n",
    "            vec[j] = count\n",
    "        return vec\n",
    "\n",
    "    def find_similar_docs(self, doc: str, n = 1):\n",
    "        '''\n",
    "        return n (default: 1) similar docs to doc based on cosine similarity\n",
    "        '''\n",
    "        cosine_vector = np.zeros(len(self.data))\n",
    "\n",
    "        a = self.__bag_of_words_vector(doc)\n",
    "\n",
    "        for i in range(len(self.data)):\n",
    "\n",
    "            b = self.__bag_of_words_vector(self.data[i])\n",
    "\n",
    "            cosine_vector[i] = self.__cosine_similarity(a,b)\n",
    "        \n",
    "        # top num docs with highest cosine similarity\n",
    "        sim = np.argpartition(cosine_vector, n)[::-1][:n]\n",
    "\n",
    "        return self.data[sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lars von trier's europa is a worthy echo of the third man, about an american coming to post-world war ii europe and finds himself entangled in a dangerous mystery.<br /><br />jean-marc barr plays leopold kessler, a german-american who refused to join the us army during the war, arrives in frankfurt as soon as the war is over to work with his uncle as a sleeping car conductor on the zentropa railway. what he doesn't know is the war is still secretly going on with an underground terrorist group called the werewolves who target american allies. leopold is strongly against taking any sides, but is drawn in and seduced by katharina hartmann (barbara sukowa), the femme fatale daughter of the owner of the railway company. her father was a nazi sympathizer, but is pardoned by the american colonel harris (eddie considine) because he can help get the german transportation system up and running again. the colonel soon enlists, or forces, leopold to be a spy (without giving him a choice or chance to think about it) to see if the werewolves might carry out attacks on the trains.<br /><br />soon, leopold is stuck in an adventure by being involved with both sides of the conflict in a mysterious and film noir-ish way, where everyone and everything is not what it seems. its amazing to watch the naive leopold deal with everything (his lover, the terrorists, the colonel, annoying passengers, his disgruntled uncle, even the railway company's officials who come to examine his work ethic) before he finally boils over and humorously and violently takes control. the film is endlessly unpredictable.<br /><br />the film stylishly shot, it always takes place at night during the winter with lots of falling snow. its shot in black and white with shots of color randomly appearing throughout. also, background screens displaying images that counter act with the images up front. add max von sydow's hypnotic narration, and europa becomes a dreamlike place that's out of this world.<br /><br />this is now a personal favorite film of mine.\n",
      "\n",
      "many people know how it feels when a loved one is lost. the feelings of pain, grief and sorrow can be unbearable. however, sometimes it is the memories they leave behind that trigger the saddest emotions. this theme is superbly portrayed in the short film 'tulip', directed by the award winning australian actress, rachel griffths. described as a movie 'as much about memories as it is about love', a string of sensitivity and sentimentality is expertly threaded into this triumphant 15 minute film.<br /><br />'tulip' is a beautifully wrought, touching and heart-warming story of a man's journey in coming to terms with the loss of his wife through the relationship he shares with a very special animal, 'tulip'. the film opens with a rising dawn, the chirping of birds and a vast landscape, introducing the sense of rustic harmony present throughout the film. a soft music plays, marking the entrance of ruth (jean bain). she wears a flowered dress and apron with a sun hat on her head. she gently pets tulip, caressing her ears and patting her back. the furnishing of the house is impressive and the attention to detail is creditable (a vase of tulips can be seen on the bench), reflecting the peaceful rural community. will (charles 'bud' tingwell) greets ruth as she is spooning the milk from the bucket. they pour the milk and coffee together, a sign of companionship and teamwork. not a word is said but it is obvious that their relationship is close and affectionate; they paint a perfect picture of happiness.<br /><br />sadly, happiness doesn't last forever. the tragic passing of ruth affects will deeply. an effective scene of fading cars highlights will's isolation and vulnerability at the end of the day of the funeral. soon he sinks into depression and becomes oblivious to his surroundings when everything seems hopeless and lost. at will's moment of despair, tulip becomes the symbol of ruth, the genuine connection will has with his late wife. it was through tulip that will learns to cope with the absence of ruth and overcome the heartrending feelings of loneliness.<br /><br />each of the characters is realistically and solidly portrayed, especially the part of will. charles 'bud' tingwell brings the character to life through personal investment. the recent loss of his own wife (audrey tingwell) is effectively reflected in his acting. every sag of his shoulder and every frown on his brow make the viewer empathize strongly with the character. the character of ruth is wonderfully carried out by jean bain. although ruth does not say a single word throughout the movie, her sweet personality and loving relationship with will are obvious. lois ramsey and kati edwards give delightful performances in supporting roles as the friendly margaret and mary. they also add a subtle humor to the bittersweet story.<br /><br />an anecdote from griffths' childhood, the story of loss and discovery is remarkably captured in 'tulip'. beautifully shot and superbly acted, this film will surely make you misty eyed, triumphant or feel like drinking a cup of milk.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# docs to find similarity with\n",
    "ir = InformationRetrieval(train_data[0][:2000])\n",
    "\n",
    "# make W list containing all unique words\n",
    "ir.vectorize_data()\n",
    "\n",
    "# doc : list ot top n documents matching the given document\n",
    "docs = ir.find_similar_docs(train_data[0][2002], 2)\n",
    "\n",
    "print(*docs, sep='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this isn't the best bigfoot ever made, but by the recent standards of nature gone awry movies, mostly showing on the sci-fi channel, this is quality stuff. it has some action, some humor, decent f/x and bigfoot. cg is used, but so are some practical f/x, which i like.<br /><br />overall this movie is worth a watch if you are a fan of b horror/sci-fi and need a fix. it's better than the movie sasquatch and not a sequel to it, so don't be fooled.<br /><br />the acting is better than you may expect to find in a movie like this and the directing is more than adequate. expect a bit of a lul as the characters are \"developed\", but know that things will pick up. if you are watching a dvd you may want to skip a chapter or two.\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][2002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guinea pig: the devil's experiment is without a doubt ***** stars on first view, its a raw realistic creepy and disturbing look into the dark side of human nature. this movie gets right to the point, you may be thinking what point? the point is to satisfy fan's of just extreme violence and gore. this movie has some gore, more or less just torturing a women violently. there are really only 3 scene's that could be considered gore. i'll tell you one thing though guinea pig: the devil's experiment makes hostile look like sesame street. if you thought hostile was a crazy brutal disturbing torture flick then you ain't seen the half of it until you've seen guinea pig: the devil's experiment.<br /><br />movie rating 0-5, gore 0-10<br /><br />guinea pig: the devil's experiment (uncut) ***** (7)\n",
      "\n",
      "\n",
      "this film deals with the atrocity in derry 30 years ago which is commonly known as bloody sunday.<br /><br />the film is well researched, acted and directed. it is as close to the truth as we will get until the outcome of the saville enquiry. the film puts the atrocity into context of the time. it also shows the savagery of the soldiers on the day of the atrocity. the disgraceful white-wash that was the widgery tribunal is also dealt with.<br /><br />overall, this is an excellent drama which is moving and shocking. when the saville report comes out, watch this film again to see how close to the truth it is.\n",
      "\n",
      "\n",
      "as long as you go into this movie knowing that it's terrible: bad acting, bad \"effects,\" bad story, bad... everything, then you'll love it. this is one of my favorite \"goof on\" movies; watch it as a comedy and have a dozen good laughs!\n"
     ]
    }
   ],
   "source": [
    "# number of documents to analyse\n",
    "\n",
    "top = 500\n",
    "\n",
    "polarities = []\n",
    "\n",
    "for doc in train_data[0][:top]:\n",
    "    polarities.append(TextBlob(doc).polarity)\n",
    "\n",
    "# number of (most +ve or most -ve) documnets to filter\n",
    "num = 3\n",
    "\n",
    "list_most_neg = np.argpartition(polarities, num)[:num]\n",
    "\n",
    "list_most_pos = np.argpartition(polarities, num)[::-1][:num]\n",
    "\n",
    "most_pos = train_data[0][list_most_pos]\n",
    "\n",
    "most_neg = train_data[0][list_most_neg]\n",
    "\n",
    "print(*most_neg, sep='\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polarities = []\n",
    "\n",
    "for doc in train_data[0]:\n",
    "    polarities.append(TextBlob(doc).polarity)\n",
    "\n",
    "pol_df = pd.DataFrame(polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000002C40C43A748>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pol_df.hist(bins = 50)\n",
    "# Plotted histogram shows that more than half of the reviews in train data have +ve polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
